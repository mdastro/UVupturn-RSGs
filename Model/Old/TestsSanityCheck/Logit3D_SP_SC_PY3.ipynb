{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(parameter):\n",
    "    \"\"\"\n",
    "    :param parameter: the array with the parameters you wish to standardize\n",
    "    :return param_mean: mean of the input array\n",
    "    :return param_std : standard deviation of the input array\n",
    "    :return stdized_param: final standardized array\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    param_mean    = np.mean(parameter)\n",
    "    param_std     = np.std(parameter)\n",
    "    stdized_param = []                  # standardized parameter -- output\n",
    "    for i in range(parameter.size):\n",
    "        param_temp = (parameter[i] - param_mean)/param_std\n",
    "        stdized_param.append(param_temp)\n",
    "    stdized_param = np.array(stdized_param)\n",
    "    return (param_mean, param_std, stdized_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def un_standardize(stdized_par, mean, std):\n",
    "    \"\"\"\n",
    "    This function undoes what the former does!\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    recovered_par = []\n",
    "    for i in range(stdized_par.size):\n",
    "        param_temp = stdized_par[i]*std + mean\n",
    "        recovered_par.append(param_temp)\n",
    "    recovered_par = np.array(recovered_par)\n",
    "    return (recovered_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api   as sm\n",
    "import seaborn           as sns\n",
    "import arviz\n",
    "import pystan\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('../../../Catalogue/binom_reg_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the dataset\n",
    "redshifts   = my_data['Z']\n",
    "logit_class = my_data['LOGIT_CLASS(1-UVUP;0-UVWEAK)'].values.astype(int)    # y axis: logit class: UVUP yes/no\n",
    "whan_class  = my_data['WHAN(0-NA;1-RP;2-wA;3-sA;4-SF)'].values.astype(int)  # My types of galaxies\n",
    "mass        = my_data['STELLAR_MASS'].values.astype(int)                    # 1st parameter\n",
    "redshift    = redshifts.values.astype(float)                                # 2nd parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original parameters\n",
    "mean_mass, std_mass, x2 = standardize(mass)\n",
    "x1      = redshift\n",
    "y       = logit_class\n",
    "classes = (whan_class+1).astype(int)        # Must sum +1 to avoid index issues with STAN\n",
    "n_obs   = x1.size\n",
    "n_class = np.unique(classes).size\n",
    "\n",
    "# new parameters - important for plotting!\n",
    "n_obs2 = 30\n",
    "x1_sim = np.linspace(x1.min(), x1.max(), n_obs2)\n",
    "x2_sim = np.linspace(x2.min(), x2.max(), n_obs2)\n",
    "\n",
    "# grid\n",
    "plot_x1, plot_x2 = np.meshgrid(x1_sim, x2_sim)  # THIS IS WHERE THE GRID IS DONE\n",
    "plot_x1 = plot_x1.reshape((n_obs2**2), 1)\n",
    "plot_x2 = plot_x2.reshape((n_obs2**2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset to be used in the regression\n",
    "regression_data      = {}                                                 # Dictionary, as stated in the pystan manual\n",
    "regression_data['Y'] = y\n",
    "regression_data['X'] = sm.add_constant(np.column_stack((x1, x1**2, x2, x2**2)))\n",
    "regression_data['K'] = regression_data['X'][0,:].size                     # Number of betas -- b0, b1, b2, b3, b4\n",
    "regression_data['W'] = classes\n",
    "regression_data['N'] = n_obs\n",
    "regression_data['C'] = n_class                                       # Number of different classes (partial pooling)\n",
    "\n",
    "# dataset to be used in the plot -- after meshgrid\n",
    "regression_data['X2'] = sm.add_constant(np.column_stack((plot_x1, plot_x1**2, plot_x2, plot_x2**2)))\n",
    "regression_data['N2'] = n_obs2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit: STAN code -----------------------------------------------------------------------------------------------------\n",
    "stan_code = \"\"\"\n",
    "// DECLARATION OF VARIABLES ------------------------------------------------------------------------------------------\n",
    "data{\n",
    "    int<lower=1> N;\n",
    "    int<lower=1> N2;\n",
    "    int<lower=1> K;\n",
    "    int<lower=1> C;\n",
    "    int W[N];\n",
    "    int<lower=0, upper=1> Y[N];\n",
    "    matrix[N, K] X;         // redshift and stellar mass \n",
    "    matrix[N2,K] X2;        // redshift and stellar mass -- after grid\n",
    "    }\n",
    "\n",
    "// DEFINING THE PRIOR(S) ---------------------------------------------------------------------------------------------\n",
    "parameters{\n",
    "    matrix[K,C] beta;       // 25 betas!\n",
    "    real<lower=0> sigma;    // Shared hyperpriors\n",
    "    real mu;                // Shared hyperpriors\n",
    "    }\n",
    "\n",
    "// MODEL: PROBABILITY, HYPERPRIORS, PRIORS, AND REGRESSION -----------------------------------------------------------\n",
    "model {\n",
    "   vector[N] prob;\n",
    "    for (i in 1:N) {\n",
    "      prob[i] = beta[1,W[i]]*X[i,1] + beta[2,W[i]]*X[i,2] + beta[3,W[i]]*X[i,3] + beta[4,W[i]]*X[i,4] + \n",
    "      beta[5,W[i]]*X[i,5];\n",
    "      }\n",
    "\n",
    "    sigma ~ gamma(0.001, 0.001);                           // shared hyperpriors\n",
    "    mu ~ normal(0, 100);                                   // shared hyperpriors\n",
    "     \n",
    "    for (i in 1:K) {\n",
    "       for (j in 1:C) beta[i,j] ~ normal(mu, sigma);       // priors\n",
    "        }\n",
    "\n",
    "    Y ~ bernoulli_logit(prob);                             // regression\n",
    "    }\n",
    "\n",
    "// DATA TO BE PLOTTED ------------------------------------------------------------------------------------------------\n",
    "generated quantities{\n",
    "    vector[N2] prob01;\n",
    "    vector[N2] eta01;\n",
    "    vector[N2] prob02;\n",
    "    vector[N2] eta02;\n",
    "    vector[N2] prob03;\n",
    "    vector[N2] eta03;\n",
    "    vector[N2] prob04;\n",
    "    vector[N2] eta04;\n",
    "    vector[N2] prob05;\n",
    "    vector[N2] eta05;\n",
    "    \n",
    "    for(j in 1:N2){\n",
    "        eta01[j] = beta[1,1]*X2[j,1] + beta[2,1]*X2[j,2] + beta[3,1]*X2[j,3] + beta[4,1]*X2[j,4] + beta[5,1]*X2[j,5];\n",
    "        eta02[j] = beta[1,2]*X2[j,1] + beta[2,2]*X2[j,2] + beta[3,2]*X2[j,3] + beta[4,2]*X2[j,4] + beta[5,2]*X2[j,5];\n",
    "        eta03[j] = beta[1,3]*X2[j,1] + beta[2,3]*X2[j,2] + beta[3,3]*X2[j,3] + beta[4,3]*X2[j,4] + beta[5,3]*X2[j,5];\n",
    "        eta04[j] = beta[1,4]*X2[j,1] + beta[2,4]*X2[j,2] + beta[3,4]*X2[j,3] + beta[4,4]*X2[j,4] + beta[5,4]*X2[j,5];\n",
    "        eta05[j] = beta[1,5]*X2[j,1] + beta[2,5]*X2[j,2] + beta[3,5]*X2[j,3] + beta[4,5]*X2[j,4] + beta[5,5]*X2[j,5];\n",
    "        prob01[j] = inv_logit(eta01[j]);\n",
    "        prob02[j] = inv_logit(eta02[j]);\n",
    "        prob03[j] = inv_logit(eta03[j]);\n",
    "        prob04[j] = inv_logit(eta04[j]);\n",
    "        prob05[j] = inv_logit(eta05[j]);\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 2500\n",
    "chains     = 3                           # HMC chains\n",
    "warmup     = 500                       # How many of the first iterations we'll ignore - burnin\n",
    "jobs       = -1                          # Run code in parallel -- see pystan documentation\n",
    "seed       = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = {}\n",
    "control['max_treedepth'] = 20\n",
    "control['adapt_delta'] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:DeprecationWarning: pystan.stan was deprecated in version 2.17 and will be removed in version 3.0. Compile and use a Stan program in separate steps.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_50fd202ecaf8992b685aa66bc38cb07a NOW.\n",
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "WARNING:pystan:Chain 2: E-BFMI = 0.132\n",
      "WARNING:pystan:Chain 3: E-BFMI = 0.0503\n",
      "WARNING:pystan:E-BFMI below 0.2 indicates you may need to reparameterize your model\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "fit = pystan.stan(model_code=stan_code, data=regression_data, seed=seed, iter=iterations, chains=chains, warmup=warmup,\n",
    "                  n_jobs=jobs, control=control)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "CPU process time: 4.07 [min]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"--------------------------------------------------\")\n",
    "print (\"CPU process time: %.2f [min]\" % float((end-start)/60))\n",
    "print (\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_to_arviz = arviz.from_pystan(fit=fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_density(data=fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_to_arviz.from_netcdf??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
